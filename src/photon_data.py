"""
package dev initialization
"""
import os, sys
import numpy as np
import matplotlib.pylab as plt
import pandas as pd

from jupydoc import DocPublisher
from lat_timing import (Main, LightCurve)

__docs__ = ['GammaData']

#from utilities import phase_plot, poiss_pars_hist

   
class GammaData(DocPublisher): 
    """
    title: Photon data setup
   
    sections:
        title_page
        data_reduction [data_load ]


    source_name: Geminga
    data_path: $HOME/work/lat/data/photons

    """
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def data_reduction(self):
        r"""Data reduction

        There are five stages:

        1. **Photon data**: Public photon data generated by the "L1" processing. This reconstruction of the actual 
        detector information provides the energy, direction, and time for each photon, recorded in the "FT1" files.
        2. **Binned photons**: The binning used here is performed by `pointlike`. Energies are reduced to 4/decade bands according to energy and event type, front or back. Thus there are 32 such bands from 100 MeV to 1 TeV. All 100M photons were extracted from the FT1 files and put into a database, with each
        photon characterized by its time and integers specifying the band and position.

        3. **Cone selection**: For the study of a single source, a cone about its position to a radius 
        of 5 or 7 degrees, is used to select the data set to use here. A *weight* is assigned to each photon.
        This uses the `pointlike` model depending on the PSF, optimized with respect to the positions and 
        spectral  characterists of all sources. The weight is the probabality that an individual photon was 
        from the source. 

        3. **cells**: This is the basic partition in time, for studying time behavior. We usually
        use seconds for this. A final input is from the effective area and exposure, to normaize counts to
        flux. Each cell is then: 
          * central time
          * time inteval
          * exposure factor
          * a set of weights. 
        <br><br>
        5. **Light curve**: Each cell is used to make a measuremnt of the signal flux, and perhaps also the
        background, by optimizing an estimator function. Here we restrict to using the likelihood derived by
        M. Kerr. Per cell, it is:
        \begin{align*}
        \log\mathcal{L}(\alpha,\beta\ |\ {{w}}) = \sum_{w}  \log \bigg( 1 + \alpha\ w + \beta\ (1-w) \bigg) 
        -\alpha S - \beta B
        \end{align*}                
        Here the likelihood is function of the relative flux parameters $\alpha$ and $\beta$, given the set of weights $w$. $S$ and $B$ are expections for the flux relative to the averages over the 
        full dataset. For our case here, we fix $\beta$ to its expected zero. $S$ is proportional 
        to the exposure, converting counts to flux. 
        <br><br>This likelihood function is approximated by a Poisson-like function.

        #### Specified source name: **{self.source_name}**

        Data file `{self.datafile}` exists? {self.fileok}
        """
        #-------------------------------------------------------------------
        # check for data
        self.datafile = f'{os.path.expandvars(self.data_path)}/{self.source_name}.pkl'
        self.fileok =  os.path.isfile(self.datafile)
        #-------------------------------------------------------------------
        self.publishme()

    def data_load(self):
        """Load data
        
        * Data source:
        {data_source}
        
        * **photon data**: 
        {photons_head}

        * **light curve**
        {self.light_curve}

        """
        #-------------------------------------------------------------------
        
        if self.fileok:
            import pickle
            with open(self.datafile, 'rb') as inp:
                t =pickle.load(inp)
                self.photons = pd.DataFrame.from_dict(t['photons'])
                self.light_curve = LightCurve(t['light_curve'])

        else:
            self.gdata = Main(name=self.source_name)
            self.photons = self.gdata.photons  
            self.light_curve = self.gdata.light_curve()

        photons_head = self.photons.head()
        #-------------------------------------------------------------------
        self.publishme()
        return ret

    def data_save(self):
        """Save the Data

        This makes a pickle of the photon data and light curve to `{outfile}`

        Format is
        ```
        lc = self.light_curve
        outdict = dict(
            source_name=self.source_name, 
            photons=self.gdata.photons.to_dict('records'),
            light_curve =
                dict(
                    rep=lc.rep,
                    edges = lc.data.edges,
                    fit_dict = lc.fit_df.to_dict('records'),
                    ),
                )   
        ```

        Read back to check: keys are
        {pkl_keys}
        """
        #-------------------------------------------------------------------
        import pickle
        outfile = f'{os.path.expandvars(self.data_path)}/{self.source_name}.pkl'
        lc = self.light_curve
        outdict = dict(
                source_name=self.source_name, 
                photons=self.gdata.photons.to_dict('records'),
                light_curve =
                    dict(
                        rep=lc.rep,
                        edges = lc.data.edges,
                        fit_dict = lc.fit_df.to_dict('records'),
                        ),
                    )        
        with open(outfile, 'wb') as out:
            pickle.dump( outdict,  out)
        # check
        with open(outfile, 'rb') as inp:
            pkl = pickle.load(inp)
        pkl_keys = list(pkl.keys())
        #-------------------------------------------------------------------           
        self.publishme()
        
        